*************************************************
*************************************************
Chapter 3: Design and Implementation 
*************************************************
*************************************************

    "Show me your flowcharts and conceal your tables,a nd I shall continue to be mystified. Show me your tables, and I won't usually need your flowcharts; they'll be obvious."
                    Frederick P. Brooks, Jr., The Mythical Man Month

    As the quotation from Brook's famous book suggests, the design of the data structures is the central decision in the creation of a program. Once the data structures are laid out, the algorithms tend to fall into place, and the coding is comparatively easy. 

    This point of view is oversimplified but not misleading. In the previous chapter we examined the basic data structures that are the building blocks of most programs. In this chapter we will combine such structures as we work through the design and implementation of a modest-sized program. We will show how the problem influences the data structures, and how the code that follows is straightforward once we have the data structures mapped out.

    One aspect of this point of view is that the choice of programming lang is relatively unimportant to the overall design. We will design the program in the abstract and then write it in C, Java, C++, Awk, and Perl. Comparing the implementations demonstrates how languages can help or hinder, and ways in which they are unimportant. Program design can certainly be colored by a language but is not usually dominated by it.

    The problem we have chosen is unusual, but in basic form it is typical of many programs: some data comes in, some data goes out, and the processing depends on a little ingenuity.

    Specifically, we are going to generate random English tezt that reads well. If we emit random letters or random words, the result will be nonsense. For example, a program that randomly selects letters (and blanks, to seperate words) might produce this:
          dkhbna ikuvnwor sfkvn skfhvbwouc jsdf
  which is not very convincing. If we weight the letters by their frequency of appearance in English tezt, we might get this:
          edtfoae tcs trder jcii ofdslnqetacp t ola
  which is not a great deal better. Words chosen from the dictionary at random don't make much more sense:
          polydactyl equatorial splashily jowl verandah circumscribe

    For better results, we need a statistical model with more structure, such as the frequency of appearance of whole phrases. But where can we find such statistics?

    We could grab a large body of English and study it in detail, but there is an easier and more entertaining approach. The key observation is that we can use any existing text to construct a statistical model of the language as used in that text, and from that generate random text that has similar statistics to the original.

    
*************************************************
3.1 The Markov Chain Algorithm 
*************************************************
    
   An elegant way to do this sort of processing is a technique called a Markov chain algorithm. If we imagine the input as a sequence of overlapping phrases, the algorithm divides each phrase into two parts, a multi-word prefix and a single suffix word that follows the prefix. A Markov chain algorithm emits output phrases by randomly choosing the suffix that follows the prefix, according to the statistics of (in our case) the original text. Three-word phrases work well -a two-word prefix is used to select the suffix word:
    set w1 and w2 to the first two words in the text
    print w1 and w2
    loop:
        randomly choose w3, one of the successors of prefix w1 w2 in the text
        print w3
        replace w1 and w2 with w2 and w3
        repeat loop

    To illustrate suppose we want to generate random text based on a few sentences paraphrased from the epigraph above, using two word prefixes:

          Show your flowcharts and conceal your tables and I will be mystified. Show your tables and your flowcharts will be obvious. (end)

    These are some of the pairs of input words and the words that follow them:

        Input Prefixes        Suffix words that follow:
          Show your               flowcharts tables
          your flowcharts         and will
          flowcharts and          conceal
          flowcharts will         be
          your tables             and and
          will be                 mystified. obvious.
          be obvious              (end)

    A Markov alg processing this text will begin by printing Show your and will when randomly pick either flowcharts or tables. If it chooses the former, the current prefix becomes your flowcharts and the next word will be and or will. If it chooses tables, the next word will be and . Thus continues until enough output has been generated or until the end-marker is encountered as a suffix.

    Our program will read a piece of English text and use a Markov chain alg to generate new text based on the frquency of appearance of phrases of a fixed len. The number of words in the prefix, which is two in our ex, is a parameter. Making the prefix shorted tends to produce less coherent prose; making it longer tends to reproduce the input text verbatim. For English text, using two words to select a third is a good compromise; it seems to recreate the flavor of the input while adding its own whimsical touch.

    What is a word? The obvious answer is a sequence of alphabetic characters, but it is desirable to elave punctuation attached to the words so "words" and "words." are different. Thus helps to improve the quality of the generated prose by letting punctuation, and therefore (indirectly) grammar, influence the word choice, although it also permits unbalanced quotes and parantheses to sneak in. We will therefore define a word as anything between white space, a decision htat places no restriction on input language and leaves punctuatuion attached to the words. Since most programming languages have facilities to split text into white-space-seperated words, this is also east to implement.

    Because of ht emethods, all words, all two-word phrases, and all three-word phrases in the output must have appeared in the input, but there should be many four-word and longer ohrases that are synthesized. here are a few sentences produced by the program we will develop in this chapter, when given the text of Chapter VII of The Sun Also Rises by Ernest Hemingway:

     As I started up the undershirt onto his chest black, and big stomach muscles bulging under the light. "You see them?" Below the line where his ribs stopped were two raised white welts. "See on the forehead." "Oh, Brett, I love you." "Let's not talk. Talking's all bilge. I'm going away tomorrow." "Tomorrow?" "Yes. Didn't I say so? I am." "Let's have a drink, then." 

    We were lucky here that punctuation came out correctly; that need not happen.


*************************************************
3.2 Data Structure Alternatives 
*************************************************

    How much input do we intend to deal with? How fast must the program run? It seems reasonable to ask our program to read ina whole book, so we should be prepared for input sizes of n = 100,000 worsd or more. The output will be hundreds or perhaps thousands of words, and the program should run in a few seconds instead of miutes. With 100,000 words of input text, n is fairly large so the algorithms cannot be too simplistic if we want the program to be fast. 
    The Markov alg must see all th einput before it can begin to generate output, so it must store the entire input in some form. One possibility is to read the whole input and store it ina  long string, but we clearly want the input broken down into words. If we store it as an array of ptrs to words, output generation is simple: to produce each words, scan the input text to see what possible suffix words follow the prefix that was just emitted, and then choose one at random. However, that measnscanning all 100,000 words for each input we generate; 1000 words of input meands hundreds of millions of string comparisons, which will not be fast.

    Another possibility is to store only unique input words, together with a list of where they appear in the input sot hat we can locate successor words more quickly. We could use a hash table lik the one in Ch02, but that version does not directly address the needs of the Markov alg, which must quickly locate all the suffixes of a given prefix.

    We need a data structure that better represents a prefix and its assocated suffixes. The program will have tow passes, an input pass tht builds the data structure representing the phrases, and an output pass that uses the data structure to generate the random outpt. In both passes, we need to look up a prefix (quicky): in the input pass to update its suffixes, and inthe output pass to to select at random from the possible suffixes. This suggests a hash table whise keys are prefixes and whose values are th esets of suffixes for the corresponding prefixes.

    For purposes of description, we will assume a two words prefix, so edach output words is based on th epari of words that precede it. The numbe rof words inthe prefix does not affect the design  and the programs should handle any prefix len, but selecting a number makes the discussion concrete. The prefix and teh set of all its possible suffixes well call a state, which is standard terminology for markog algs.

    Given a prefix, we need to store all the suffixs that follow it so we can access them later. The suffixes are unordered and added one at a time, WE don't know how many there will be , so we need a data structure that grows easily and efficiently, such as a list or a dynamic array. When we are generating output, weneed to be able to choose one suffix at random from th eset of suffixes associated with a particulare prefix. Items are never deleted.

    What happends if a phrase appears more than once? This can be represented by putting "twice" twice in the usffix list for might appear or by puttinf it in once, with an associated counter set to 2. We have tried and without counters; without is easier since adding a suffix does no require checking whther it is there already, and expresiments shows that the difference in run time was negligible.

  In summary each state comprises a prefix and a luffix. The information is stored a hash table, with preafix as key. Each prefix is a fixed size set of words. If a suffix occurs more than once for a griven prefix, each occurrence will be included sparately in the list.

    The next decision is how to represent th ewords themselves. The easy way is to store them as indv strings. since most text has many words appearin


*************************************************
3.3 Building the data structures in C 
*************************************************

    Define some constants:
        enum {
            NPREF   = 2,    // number of prefix words
            NHASH   = 4093, // size of state hash table array
            MAXGEN  = 10000 // maximum words generated
        };

    If NPREF is a compile time constant rather than a run time variable, storage management is simpler. The array size is set failry largebecause we expect to give the program large input documents, perhaps a whole book. WE chose NHASH = 4093 so that if the input has 10,000 distinct prefixes (word pairs), the average chain will be very short, two or three prefixes. The larger the size the shorter the expected len of the chains and thus faster the lookup. This program is really a toy so the performance isnt critical but if we make the array too small the program will not handle out expected input in reasonable time; on the other hand, if we make it too big it might not fit in the available mem

    The prefix can be stored as an array of words. The elements of the hash table will be represented as a State data type, associating the Suffix list with the prefix:

        typedef struct State State;
        typedef struct Suffix Suffix;
        struct state { // prefix + suffix list
            char    *pref[NPREF]; // prefix words
            Suffix  *suf;         // list of suffixes
            State   *next;        // next in hash table   
        };

        struct Suffix { // list of suffixes
            char    *word;    // suffix
            Suffix  *next;    // next in list of suffixes
        };

        State   *statetab[NHASH];   // hash table of states

    Pictorally, the data structures look like this:

  statetab:
  
  --------
  |      |   a State:          ----------
  --------   ---------     /-->| "Show" |
  |    --|-->|pref[0]|----/    ----------
  --------   ---------           ----------
  |      |   |pref[1]|---------->| "your" |
  --------   ---------           ----------
  |      |   |  suf  |-----\        a suffix:
  --------   ---------      \       ---------    ----------------
  |      |   | next  |       \----->| word  |--->| "flowcharts" |  
  --------   ---------              ---------    ----------------
  |      |        \                 | next  |--\   another suffix:
  --------         \                ---------   \     ---------    ----------
  |      |          \                            \--->| word  |--->|"tables"|   
  --------   another state:                           ---------    ----------
  |      |            \                               | next  |
  -------         ---------                           ---------
                  |pref[0]|
                  ---------
                  |pref[1]|
                  ---------
                  |  suf  |
                  ---------
                  | next  |
                  ---------

    We need a hash fn for prefixes, whcih are arrays of strs. It is simple to modify the string hash fn form chapter 2 to loop over the strings in the array, thus in effect hashing the concatentation of hte strings

        // hash: compute hash val for array og NPREF strs
        unsigned int hash(char *s[NPREF])
        {
            unsigned int h;
            unsigned char *p;
            int i;

            h = 0;
            for (i = 0; i < NPREF; i++)
                for (p = (unsigned char *) s[i]; *p != '\0'; p++)
                    h = MULTIPLIER * h + *p;

            return h % NHASH;
        }

    A similar modification to the lokup routine completes the implementation of the hash table;

        // lookup: search for prefix; create if requested.
        //    returns ptr if present or created; NULL if not
        //    creation doesnt strdup so strings mustn change later
        State* lookup(char *prefix[NPREF], int create)
        {
            int i, h;
            State *sp;

            h = hash(prefix); 
            for (sp = statetab[h]; sp != NULL; sp = sp->next) {
                for (i = 0; i < NPREF; i++)
                    if (strcmp(prefix[i], sp->pref[i]) != 0)
                        break;
                if (i == NPREF)     // found it
                    return sp;
            }
            if (create) {
                sp = (State *) emalloc(sizeof(State));
                for (i = 0; i < NPREF; i++)
                    sp->pref[i] = prefix[i];
                sp->suf = NULL;
                sp->next = statetab[h];
                statetab[h] = sp;
            }
            return sp;
        }

    Notice that lookup does not make a copy of ht eincoming strings when it creates a new state; it just stores pointers in sp->pref[]. Callers of lookup must guarantee that the data wont be overwritten later. For example if the strings are in an IO buffer a copy must be made before lookup is called, otherwise subsequent input could overwrite the data that the hash table points to. Decision about who owns a resource shared accross and interface arise often. We will expore this topic at lenths in the next chapter.

    Next we need to build a hash table as the file is read:
      
        // build: read input, build prefix table
        void build(char *prefix[NPREF], FILE *f)
        {
            char buf[100], fmt[10];
            
            // create a format string since just %s could overflow buf
            // evaluates to "%{number}s" dynamically based on the bufsize-1 
            sprintf(fmt, "%%%ds", sizeof(buf)-1); 
            while (fscanf(f, fmt, buf) != EOF)
                add(prefix, estrdup(buf));
        }

    The peculiar call to sprintf gets aronud and irritating problem with fscanf, which is otherwise perfect for the job. A call to fscanf with format %s will read the next which space delimited word from the file into the buffer, but there is no limit on size: a long word might overflow the input buffer, wreaking havoc. If the buffer is 100 bytes long (which is far beyond what we expect ever to appear in normal text), we can use format %99s (leaving on byte for the terminal '\0'), which tells fscanf to stop after 99 bytes. A long word will be broken into pieces, which is unfortunate but safe. We could declare:

      enum  { BUFSIZE = 100};
      char  fmt[] = "%99s"; // bufsize - 1;
  but that requires two contants for one arbitrary decision -the size of the buffer- and introduces the need to maintain their relationship. The problem can be solved once and for all by creating the format srting dynamicaly with sprintf, so that is the approach we take.

    The two args to add are the prefix array holding the previous NPREF words of input and a FILE pointer. It passes the prefix and a copy of the input word to add, which adds the new entry to the hash table and advances the prefix:
      // add: add words to suffix list, update prefix
      void add(char *prefix[NPREF], char *suffix)
      {
          State *sp;
          sp = lookup(prefix, 1); // create of not found
          addsuffix(sp, suffix);
          // move the words down the prefix
          memmove(prefix, prefix+1, (NPREF-1)*sizeof(prefix[0]));
          prefix[NPREF-1] = suffix;
      } 

    The call to memmove is the idiom for deleting from an array. It shifts elements q through NPREF-1 in the prefix down to positions 0 through NPREF-2, deleting the first prefix word and opening a space for a new one at the end.
    The addsuffix routine adds the new suffix:
      // addsuffix: add to state, suffix must not change later
      void addsuffix(State *sp, char *suffix)
      {
          Suffix *suf;
          
          suf = (Suffix *) emalloc(sizeof(Suffix));
          suf->word = suffix;
          suf->next = sp->suf;
          sp->suf = suf;
      }

    We split the action of updating the state into two fns: add performs the general service of adding a suffix to a prefix, while addsuffix preforms the implementation specific action of adding a word to a suffix list. The add routine is used by build, but addsuffix is used internally only by add; it is an implementation detail that might change and it seems better to have it in a separate function even though it is called in only one place.


*************************************************
3.4 Generating Output
*************************************************

    With the data structure built, the next step is to generate the output. The basc idea is as before: given a prefix, select one of its suffixes at random, print it, then advance the prefix. This is the steady state of processing; we must still figure out how to start and stop the algorithm. Starting is easy if we remember the words of the first prefix and begin with them. Stopping is easy too. We need a marker word to terminate the alg. After all the regular input, we can add a terminator, a "word" that is guaranteed not to appear in any input.
      build(prefix, stdin);
      add(prefix, NONWORD);

    NONWORD should be some value that will never be encountered in regular input.Since the input words are delimited by white space, a "word" of white space will serve, such as a newline char:

      Char NONWORD[] = '\n';  // cannot appear as real word

    One more worry: what happens if there is insufficient input to start the alg? There are two approaches to this sort of problem, either exit prematurely if there is insufficient input. or arrange that there is always enough and don't bother to check. IN this program, the latter approach workds well.

    We can initialize building and generating with a fabricated prefix, which guarantees that there is alwas enough input for hte program. To prime the loops, initialize the prefix array to be all NONWOR words. Thus has the nice benefit that the first word of the input file will be the first suffix of the fake prefix, so the generation loop needs to pring only the suffixes it produces.

    In case the output is unmanageably long, we can terminate the alg affter some num of words are produced or when we hit NONWWORD as  asuffix, whichecer comes first.

    Addinf a few NONWORDS to the ends of the data simplifies the main processing loops of the program significantly; it is an example of the technique cof adding sentinel values to mark boundaries.

    As a rule, try to handle irregularities and exceptions and special cases in data. Code is harder to get right so the control flow should be as simple and regular as possible.

    The generate fn uses the alg we sketched orginally. It produces one word per line of output, which can be grouped in to longer lines iwht a word processor; Ch09 shows a simple formatter called fmt for this task

    With the use of the initial and final NONWORD strings, generate starts and stops properly:

        // generate: produce output. on word per lin
        void generate(int nwords)
        {
            State *sp;
            Suffix *suf;
            char *prefix[NPREF], *w;
            int i, nmatch;

            for (i = 0; i < NPREF; i++) // reset initial prefix
                prefix[i] = NONWORD;

            for (i = 0; i < nwords; i++) {
                sp = lookup(prefix, 0);
                nmatch = 0;
                for (suf = sp->suf; suf != NULL; suf = suf->next)
                    if (rand() % ++nmatch == 0)  // prob = 1/nmatch
                        w = suf->word;
                if (strcmp(w, NONWORD) == 0)
                    break;
                printf("%s\n", w);
                // memmove is the idiomatic way in C for
                // of shifting all values in an array
                memmove(prefix, prefix+1, (NPREF-1)*sizeof(prefix[0]));
                prefix[NPREF-1] = w;
            }
        }

    Notice the alg for selecting one item at random when we dont know how many items there are. The var nmatch counts the number of matches as the list is canned. The expression
          rand() % ++nmatch == 0
    increments nmatch and is then true with prob 1/nmatch;
    Thus the first matching item will set w with a 100% chance
    the second matching item 50%
    the third 33%
    etc
  At any time, each one of hte k matching items in the suffix list has been selected with probability 1/k

    At the beginning we set the prefix to the starting value, which is guaranteed to be installed in the hash table. The first Suffix values we find will be the first words of the document, since they are the unique follow-on to the starting prefix. After that, random suffixes will be chosen. The loop calls lookup to find the hash table entry for the current prefix, then chooses a random suffix, prints it, and advances the prefix.

    If the suffix we choose is NONWORD, we're one, because we have chosen the state that corresponds to the end of th e input. If the suffix is not NONWORD, we print it, then drop the first word of the prefix with a call to memmove, promote the suffix to be the last word of the prefix, and loop. (memmove is used to shift all values of an array).

    Now we can put all this together into a main routine that reads the stdin and generates at most a specified number of words:
        // markov main: markov-chain random text generation
        int main(void)
        {
            int i, nwords = MAXGEN;
            char *prefix[NPREF];          // current input prefix

            for (i = 0; i < NPREF; i++)   // set up initial prefix
                prefix[i] = NONWORD;
            build(prefix, stdin);
            add(prefix, NONWORD);
            generate(nwords);
            return 0;
        }

    This completes out C implementation. We will return at the end of the chaoter to a comparison of programs in different languages. The great strengths of C are that it gives the programmer compelte control over the implementation, and programs written in it tend to be fast. The cost, however, is that the C programmer must do more of th ework, allocating and reclaiming memory, creating hash tables and linked lists, and the like. C is a razor sharp tool, with which one can create an elegant and efficient program or a bloody mess.

  Ex 3-1 The alg for selecting a random item from a list of unknown length depends on having a good random number generator. Design and carry out experiments to determine how well the method works in practice.

  Ex 3-2 If each input word is stored in a second hash table, the text is only stored once, which should save space. Measure some documents to estimate how much. This organization would allow us to compare pointers rather than strings in the hash chains for prefixes, which should run fsater. Implement this version and measure the change in speed and memory consumption.

  Ex 3-3 Remove the statements that place sentinel NONWORDs at the beginning and end of the data, and modify generate so it starts and stops properly without them. Make sure it produces correct output for input with 0, 1, 2, 3, and 4 words. Compare this implementation to the version using setinels.


*************************************************
3.5 Java
*************************************************

    Our second implementation of the Markov chain algorithm is in Java. Object-oriented languages like Java encourage one to pay particular attention to the interfaces between the components of the program, which are then encapsulated as independent data items called objects or classes, with associated functions called methods.
  
    Java has a richer library than C, including a set of container classes to group existing objects in various ways. One example is the Vector class, which provides a dynamically growable array that can store any Object type. Another example is the Hashtable class, with which one can store and retrieve values of one type using objects of another type as keys.

    In out application, Vectors of strings are the natural choice to hold prefixes and suffixes. We can use a Hashtable whose keys are prefix vectors and whose values are suffix vectors. The terminology for this type of construction is a map from prefixes to suffixes; in Java, we need no explicit State type becuase Hashtable implicitly connects (maps) prefixes to suffixes. This design is different from the C version, in which we installed State structures that held both prefix and suffix lists, and hashed on the prefix to revocer the full state.

    A  Hashtable provides a put method to store a key-value pair, and a get emthod to retrieve the value for a key:
          Hashtable h = new Hashtable();
          h.put(key, value);
          Sometype v = (Sometype) h.get(key);

    Our implementation has three classes. The first class, Prefix, holds the words of the prefix:
          class Prefix {
              public Vector pref;   // Npref adjacent words from input
          }

    The second class, Chain, reads the input, builds the hash table, and generates the output; here are its class variables:
          class Chain {
              static final int NPREF = 2;   // size of prefix
              static final String NONWORD = "\n"; // word that cannot appear

              Hashtable statetab = new Hashtable(); // key = Prefix, value = Suffix
              Prefix prefix = new Prefix(NPREF, NNOWORD);
              Random rand =  new Random();
              ...
          }
    The third class is the public interface; it holds main and instantiates a Chain:
          class Markov {
              static final int MAXGEN = 10000;
              public static void main(String[] args) throws IOException
              {
                  Chain chain = new Chain();
                  int nwords = this.MAXGEN;
                  
                  chain.build(System.in); 
                  chain.generate(nwords);
              }
          }
    
    When an instance of class Chain is created, it in turn creates a hash table and sets up the initial prefix of NPREF NONWORDs. The build fn uses the library fn StreamTokenizer to parse the input into words seperated by white space characters. The three calls before the loop set the tokenizer intot he proper state for our definition of "word."

          // Chain build: build State table from input stream:
          void build(InputStream in) throws IOException
          {
              StreamTokenizer st = new StreamTokenizer(in);
            
              st.resetSyntax();                     // remove default rules
              st.wordChars(0, Character.MAX_VALUE); // turn on all chars
              st.whitespaceChars(0, ' ');           // except up to blank
              while (st.nextToken() != st.TT_EOF)
                  add(st.sval);
              add(this.NONWORD);
          }

    The add fn retrieves the vector of suffixes for the current prefix form the hash table; if there are none (the vector is null), add creates a new vector and a new prefix to store in the hash table. In either case, it adds the new word to the suffix vector and advances the prefix by dropping the first word and adding the new word at the end.

          // Chain add: add word to suffix list, update prefix
          void add(String word)
          {
              Vector suf = (Vector) statetab.get(this.prefix);

              if (suf == null) {
                  suf = new Vector();
                  statetab.put(new Prefix(this.prefix))
              }
              suf.addElement(word);
              prefix.pref.removeElementAt(0);
              prefix.pref.addElement(word);
          }

    Notice that if suf is null, add installs a new Prefix in the hash table, rather than prefix itself. This is because the Hashtable class stores items by reference, and if we don't make a copy, we would inadvertantly overwrite data in the table. This is the same issue we had to deal with in the C program.

    The generation fn is similar to the C version, but slightly more compact becuase it can index a random vector element directly isntead of looping through a list.

          // Chain generate: generate output words
          void generate(int nwords)
          {
              this.prefix = new Prefix(this.NPREF, this.NONWORD);
              for (int i = 0; i < nwords; i++) {
                  Vector s = (Vector) statetab.get(this.prefix);
                  int r = Math.abs(rand.nextInt()) % s.size;
                  String suf = (String) s.elementAt(r);
                  if (suf.equals(NONWORD))
                      break;
                  System.out.println(suf);
                  prefix.pref.removeElementAt(0);
                  prefix.pref.addElement(suf);
              }
          }

    The two constructors of Prefix create new instances from supplied data. The first copies and eixsting Prefix, and the second created a prefix from n copies of a string; we use it to make NPREF copies of NONWORD when initializing:
          
          // Prefix constructor: duplicate existing prefix
          Prefix(Prefix p)
          {
              this.pref = (Vector) p.pref.clone();
          }

          // Prefix constructor: n copies of str
          Prefix(int n, String str)
          {
              this.pref = new Vector();
              for (int i = 0; i < n; i++)
                  this.pref.addElements(str);
          }

    Prefix also has two methods, hashCode and equals, that are called implicitly by the implementation of Hashtable to index and search the table. It is the need to have an explicit class for these two methods for Hashtable that forced us to make Prefix a full-fledged class, rather than just a Vector like the suffix.
  
    The hashCode method builds a single hash value by combining the set of hashCodes for the elements of the vector:
          
          static final int MULTIPLIER = 31;     // for hashCode()

          // Prefix hashCode: generate hash from all prefix words
          public int hashCode()
          {
              int h = 0;
              
              for (int i = 0; i < pref.size(); i++)
                  h = this.MULTIPLIER * h + this.pref.elementAt(i).hashCode();
              return h;
          }
    and equals does an elementwise comparison of the words in two prefixes:
          
          // Prefix equals: compare two prefixes for equal words
          public boolean equals(Object o)
          {
              Prefix p = (Prefix) o;
              
              for (int i = 0; i < pref.size(); i++)
                  if (!this.pref.elementAt(i).equals(p.pref.elementAt(i)))
                      return false;
              return true;
          }  

    The Java program is significantly smaller than the C program and takes car of more details; Vectors and Hashtable are obvioud examples. In general, storage amangement is easy since vectors grow as needed and grabage colelction takes care of reclaiming memory that is no longer referenced. But to use the Hashtable class, we still need to write functions hashCode and equals, so Java isn't taking care of all the details.

    Comparing the way the C and Java programs represent and operate on the same basic data structure, we see that the Java version has better separation of functionality. For example, to switch from Vectors to arrays would be easy. In the C version, everything knows what everything else is doing: the hash table operates on arrays that are maintained in various places, lookup knows the layout of the State and Suffix structures, and everyone knows the size of the prefix array.
    %  java Markov < jr_chemistry.txt | fmt
    Wash the blackboard. Watch it dry. The water goes into the air. When water goes into the air it evaporates.
    % 

  Ex 3-4 Revise the Java version of markov to use an array instead of a vector for the prefix in the State class



*************************************************
3.6 C++
*************************************************

    Our third implementation is in C++. Since C++ is almost a superset of C, it can be used as if it were C with a few notational conveniencies, and out original C version of markov is also a legal C++ program. A more appropriate use of C++, however, would be to define classes for the objects in the program, more or less as we did in Java; this would let us hide implementation details. We decided to go even further by using the Standard Template Librarr, or STL, since STL has built-in mechanisms that will do much of what we need. The ISO standard for C++ includes the STL as part of the language definition.

    The STL provides containers such as vectors, lsits, and sets, and a family of fundamental algos for searching, sorting, inserting, and deleting, Using the template features of C++, every STL alg works on a variety of containers, including both user-defined types and built-in types like integers. Containers are expressed as C++ templates that are instantiated for specific data types; for example, there is a vector container that can beused to make articular types like vector<int> or vector<string>. All vector operations, including the standard algs for sorting, can be used on such data types.

    IN addition to a vector container that is similar to Java's Vector, the STL provudes a deque container. A deque (pronounced "deck") is a double-ended queue that matches what we do with prefixes: it holds NPREF elements, and lets us pop the first element and add a new one ot the end, in O(1) time for both. The STL deque is more general than we need, since it permits push and pop at either end, but performance guarantees make it an obvious choice.

    The STL also provides an explicit map container, based on balanced trees, taht stores key-value pairs and provides O(logn) retrieval of values associated with any key. Maps might not be as efficient as O(1) hash tables, but it is noce not to have to write any code whatsoever to used them. (Some non-standard C++ libraries include a hash or hash_map container whose performance may be better.)

    We also use the build in comparison functions, which in this case will do string comparisons using the individual strings in the prefix.
    
    With these components in hadn, the code goes together smoothly. HEre are the declarations:

        typedef deque<string> Prefix;
        map< Prefix, vector<string> > statetab;   // prefix -> suffixes

    The STL provides a tempalte for dequeus; the notation deque<string> specializes it to a deque whose elements are of type string. Since this type appears several times in the program, we usde a typedef to give it the name Prefix. The map type that stores the prefixes and suffixes occurs only once, however, so we did not give it a separate name; the map declaration declares a variable statetab that is a map from prefixes to vectors of strings. This is more convenient than either C or JAva, because we don't need to provide a hash fn or equals method.

    The main routine initializes the prefix, reads the input (form std input, called cin in the C++ iostream library, adds a tail, and generates the output, exactly as in the earlier versions:

        // markov main: markov-chain random text generation
        int main(void)
        {
            int nwords = MAXGEN;
            Prefix prefix;            // current input prefix

            for (int i = 0; i < NPREF; i++)
                add(prefix, NONWORD);
            build(prefix, cin);
            add(prefix, NONWORD);
            generate(nwords);
            return 0;
        }

    The fn build uses the iostream lib to read th einput one word at a time:

        // build: read input words, build state table
        void build(Prefix& prefix, istream& in)
        {
            string buf;

            while (in >> buf)
                add(prefix, buf);
        }

    The string buf will grow as necessary to handle input words of arbitrary len
    
    The add fn shows more of the advantage of using the STL:

        // add: add word to Suffix list, update prefix
        void add(Prefix& prefix, const string& s)
        {
            if (prefix.size() == NPREF) {
                statetab[prefix].push_back(s);
                prefix.pop_front();
            }
            prefix.push_back(s);
        }

    Quite a bit is going on under these apparently simple statements. The map container overloads subscripting (the [] operator) to behave as a lookup operation. The expression statetab[prefix] does a lookup in statetab with prefix as key and returns a reference to the desired entry; the vector is create if it does not exists alread. The push_back member fns of vector and deque push a new string onto the back end of the vector or deque; pop_front pops the first elem of the deque.

    Generation is similar to the previous version:

        // generate: produce output. one word per line
        void generate(int nwords)
        {
            Prefix prefix;  
            int i;
      
            for (i = 0; i < NPREF); i++) // reset initial prefix
                add(prefix, NONWORD);
      
            for (i = 0; i < nwords; i++) {
                vector<string>& suf = statetab[prefix];
                const string& w = suf[rand() % suf.size()];
                if (w == NONWORD) 
                    break;
                cout << w << "\n";
                prefix.pop_front();   //advance
                prefix.psuh_back(w);
            }
        }

    Overall, this version seems especially clear and elegant --the code is compact, the data structure is visible and the alg is completely transparent. Sadly, there is a price to apy: this version runs much slower than the original C version, though it is not the slowest. We'll come back to performance measurements shortly.

  Ex 3-5 The great strength of the STL is the eae with which one can experiment with different data structure. Modify the C++ version of Markov to use various structures to represent prefix, suffix, list, and state table. How does performance change for the different structures?
  Ex 3-6 Write a C++ version that uses only classes and the string data type but no other advanced library facilities. Compare it in style and speed to the STL versions.



*************************************************
3.7 Awk and Perl
*************************************************
    
    (I only read this section and did not write along as usual)

  Ex 3-7 Modify the Awk and Perl versions to handle prefixes of any length. Experiment to determine what effects this change has on performance.



*************************************************
3.8 Performance
*************************************************


*************************************************
3.9 Lessons 
*************************************************

  Ex 3-8 We have seen versions of the Markov program in a wide variety of languages, including Scheme, Tcl, Prolog, Python, Generic Java, ML, and Haskell; each presents its own challenges and advantages. Implement the program in your favorite language and comapre its general flavor and performance.
    Not that it is my fav lang, heaven forbid, I will provide a JS implementation of the algorithm.
  See markov.js, example usage: sh> node markov.js < file.txt
